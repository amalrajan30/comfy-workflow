{
  "name": "Z-Image Turbo + Qwen Layered GGUF Pipeline",
  "description": "3-stage pipeline: Z-Image Turbo (image gen) -> Qwen-Image-Layered GGUF (layer decomposition) -> Qwen2.5-VL GGUF (analysis). All Unsloth models.",
  "dockerImage": "ghcr.io/amalrajan30/comfy-workflow:latest",
  "containerDiskInGb": 30,
  "volumeInGb": 50,
  "volumeMountPath": "/runpod-volume",
  "ports": "8000/http",
  "env": [
    {
      "key": "MODEL_DIR",
      "value": "/runpod-volume/models"
    },
    {
      "key": "HF_TOKEN",
      "value": ""
    }
  ],
  "minVcpuCount": 4,
  "minMemoryInGb": 32,
  "gpuTypeIds": [
    "NVIDIA RTX A5000",
    "NVIDIA RTX 4090",
    "NVIDIA A40",
    "NVIDIA L40",
    "NVIDIA L40S",
    "NVIDIA A100-SXM4-80GB",
    "NVIDIA A100 80GB PCIe",
    "NVIDIA H100 PCIe",
    "NVIDIA H100 SXM"
  ],
  "readme": "## Pipeline Server\n\n### Endpoints\n- `POST /generate` — Full pipeline (text -> image -> layers -> analysis)\n- `POST /generate-image-only` — Z-Image Turbo only\n- `POST /decompose` — Qwen-Image-Layered GGUF only\n- `POST /analyze` — Qwen2.5-VL GGUF only\n- `GET /health` — Check model load status\n\n### Models downloaded on first boot (~19.5 GB, cached on volume)\n- `unsloth/Qwen2.5-VL-7B-Instruct-GGUF` (Q4_K_M + mmproj)\n- `unsloth/Qwen-Image-Layered-GGUF` (Q4_K_M)\n- `Tongyi-MAI/Z-Image-Turbo` (auto-downloaded by diffusers)\n\n### Minimum GPU: 24 GB VRAM (RTX 4090 / A5000 / L40)\n### Recommended: 48+ GB VRAM (A40 / L40S / A100)"
}
